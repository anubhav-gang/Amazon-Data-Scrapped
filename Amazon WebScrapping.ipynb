{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (155332194.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install requests\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install requests\n",
    "pip install beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anubh\\AppData\\Local\\Temp\\ipykernel_2508\\1315281187.py:64: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  asin = soup.find('th', text='ASIN')\n",
      "C:\\Users\\anubh\\AppData\\Local\\Temp\\ipykernel_2508\\1315281187.py:74: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  manufacturer = soup.find('th', text='Manufacturer')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraped data saved to amazon_products_with_details.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "def scrape_amazon_products(url, pages_to_scrape=20, max_products=200):\n",
    "    headers = {\n",
    "        'User-Agent': 'Your-User-Agent-String-Goes-Here'\n",
    "    }\n",
    "\n",
    "    all_products = []\n",
    "    products_scraped = 0\n",
    "    \n",
    "    for page_number in range(1, pages_to_scrape + 1):\n",
    "        if products_scraped >= max_products:\n",
    "            break\n",
    "        \n",
    "        print(f\"Scraping page {page_number}...\")\n",
    "        page_url = url + f\"&page={page_number}\"\n",
    "        response = requests.get(page_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "\n",
    "            for product in products:\n",
    "                if products_scraped >= max_products:\n",
    "                    break\n",
    "                \n",
    "                product_info = {}\n",
    "\n",
    "                # Product URL\n",
    "                product_link = product.find('a', {'class': 'a-link-normal'})\n",
    "                if product_link:\n",
    "                    product_info['URL'] = 'https://www.amazon.in' + product_link['href']\n",
    "                    # Scraping additional details from the product page\n",
    "                    product_details = scrape_product_details(product_info['URL'])\n",
    "                    product_info.update(product_details)\n",
    "\n",
    "                    all_products.append(product_info)\n",
    "                    products_scraped += 1\n",
    "\n",
    "            # Amazon might block requests if made too frequently\n",
    "            time.sleep(2)  # Add a delay of 2 seconds between requests to avoid being blocked\n",
    "\n",
    "    return all_products\n",
    "\n",
    "def scrape_product_details(product_url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Your-User-Agent-String-Goes-Here'\n",
    "    }\n",
    "\n",
    "    product_info = {}\n",
    "    response = requests.get(product_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Description\n",
    "        description = soup.find('div', {'id': 'productDescription'})\n",
    "        if description:\n",
    "            product_info['Description'] = description.text.strip()\n",
    "\n",
    "        # ASIN\n",
    "        asin = soup.find('th', text='ASIN')\n",
    "        if asin:\n",
    "            product_info['ASIN'] = asin.find_next_sibling('td').text.strip()\n",
    "\n",
    "        # Product Description\n",
    "        product_desc = soup.find('div', {'id': 'feature-bullets'})\n",
    "        if product_desc:\n",
    "            product_info['Product_Description'] = product_desc.text.strip()\n",
    "\n",
    "        # Manufacturer\n",
    "        manufacturer = soup.find('th', text='Manufacturer')\n",
    "        if manufacturer:\n",
    "            product_info['Manufacturer'] = manufacturer.find_next_sibling('td').text.strip()\n",
    "\n",
    "    return product_info\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['URL', 'Name', 'Price', 'Rating', 'Reviews', 'Description', 'ASIN', 'Product_Description', 'Manufacturer']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for product in data:\n",
    "            writer.writerow(product)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    amazon_url = \"https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1\"\n",
    "    scraped_data = scrape_amazon_products(amazon_url, pages_to_scrape=20, max_products=200)\n",
    "\n",
    "    # Save the scraped data to a CSV file\n",
    "    save_to_csv(scraped_data, 'amazon_products_with_details.csv')\n",
    "\n",
    "    print(\"Scraped data saved to amazon_products_with_details.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anubh\\AppData\\Local\\Temp\\ipykernel_2508\\859534781.py:19: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  asin_tag = soup.find('th', text='ASIN')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product data extracted and saved to extracted_product_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def extract_product_data(url):\n",
    "    product_data = {'URL': url, 'Description': '', 'ASIN': '', 'Product_Description': '', 'Manufacturer': ''}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Description\n",
    "            description_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "            if description_tag:\n",
    "                product_data['Description'] = description_tag.get('content', '')\n",
    "\n",
    "            # ASIN\n",
    "            asin_tag = soup.find('th', text='ASIN')\n",
    "            if asin_tag:\n",
    "                asin_value = asin_tag.find_next('td').text.strip()\n",
    "                if asin_value != '-':\n",
    "                    product_data['ASIN'] = asin_value\n",
    "\n",
    "            # Product Description\n",
    "            product_desc_tag = soup.find('div', {'id': 'productDescription'})\n",
    "            if product_desc_tag:\n",
    "                product_data['Product_Description'] = product_desc_tag.text.strip()\n",
    "\n",
    "            # Manufacturer\n",
    "            manufacturer_tag = soup.find('a', {'id': 'bylineInfo'})\n",
    "            if manufacturer_tag:\n",
    "                product_data['Manufacturer'] = manufacturer_tag.text.strip()\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data from {url}: {e}\")\n",
    "    \n",
    "    return product_data\n",
    "\n",
    "def process_product_urls(input_csv, output_csv, max_urls=200):\n",
    "    with open(input_csv, 'r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = ['URL', 'Description', 'ASIN', 'Product_Description', 'Manufacturer']\n",
    "        \n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            url_count = 0\n",
    "            for row in reader:\n",
    "                if url_count >= max_urls:\n",
    "                    break\n",
    "                \n",
    "                product_data = extract_product_data(row['URL'])\n",
    "                writer.writerow(product_data)\n",
    "                url_count += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = 'amazon_products.csv'\n",
    "    output_csv = 'extracted_product_data.csv'\n",
    "    process_product_urls(input_csv, output_csv, max_urls=200)\n",
    "    print(f\"Product data extracted and saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to merged_data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def merge_csv_data(details_csv, products_csv, merged_csv):\n",
    "    # Read fieldnames from both CSV files\n",
    "    details_fieldnames = set()\n",
    "    with open(details_csv, 'r', newline='', encoding='utf-8') as details_file:\n",
    "        details_reader = csv.DictReader(details_file)\n",
    "        details_fieldnames.update(details_reader.fieldnames)\n",
    "\n",
    "    products_fieldnames = set()\n",
    "    with open(products_csv, 'r', newline='', encoding='utf-8') as products_file:\n",
    "        products_reader = csv.DictReader(products_file)\n",
    "        products_fieldnames.update(products_reader.fieldnames)\n",
    "\n",
    "    # Determine common fieldnames\n",
    "    common_fieldnames = details_fieldnames.intersection(products_fieldnames)\n",
    "\n",
    "    # Read data from amazon_products_with_details.csv\n",
    "    details_data = {}\n",
    "    with open(details_csv, 'r', newline='', encoding='utf-8') as details_file:\n",
    "        details_reader = csv.DictReader(details_file)\n",
    "        for row in details_reader:\n",
    "            details_data[row['URL']] = row\n",
    "\n",
    "    # Read data from amazon_products.csv and merge\n",
    "    merged_data = []\n",
    "    with open(products_csv, 'r', newline='', encoding='utf-8') as products_file:\n",
    "        products_reader = csv.DictReader(products_file)\n",
    "        for row in products_reader:\n",
    "            url = row['URL']\n",
    "            if url in details_data:\n",
    "                merged_row = {field: details_data[url].get(field, '') for field in common_fieldnames}\n",
    "                merged_row.update(row)\n",
    "                merged_data.append(merged_row)\n",
    "\n",
    "    # Write merged data to a new CSV file\n",
    "    with open(merged_csv, 'w', newline='', encoding='utf-8') as merged_file:\n",
    "        writer = csv.DictWriter(merged_file, fieldnames=sorted(list(common_fieldnames) + ['Description', 'ASIN', 'Product_Description', 'Manufacturer']))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(merged_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    details_csv = 'amazon_products_with_details.csv'\n",
    "    products_csv = 'amazon_products.csv'\n",
    "    merged_csv = 'merged_data.csv'\n",
    "    merge_csv_data(details_csv, products_csv, merged_csv)\n",
    "    print(f\"Merged data saved to {merged_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
